{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007aa1d5-6f04-4f89-972e-847c597bb04e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !rm -rf ~/.cache/huggingface/datasets\n",
    "# !pip install -U datasets\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298a5bc5-bfaa-42b0-81bf-ba221a4673ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('./data_for_paper/train_en_report.csv', index_col=False, encoding='utf-8')\n",
    "train, valid = train_test_split(df, test_size=0.1)\n",
    "train = train.reset_index(drop=True)\n",
    "valid = valid.reset_index(drop=True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a23f87-f449-4fa4-82c8-e849f04c1bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train= train.rename(columns={\"input\": \"question\", \"output\": \"answer\"})\n",
    "# valid =valid.rename(columns={\"input\": \"question\", \"output\": \"answer\"})\n",
    "# test = test.rename(columns={\"input\": \"question\", \"output\": \"answer\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df624890-0765-453b-a46a-703cb373f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tiktoken\n",
    "from collections import Counter\n",
    "import math\n",
    "#from kiwipiepy import Kiwi\n",
    "import os\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import math\n",
    "from kiwipiepy import Kiwi\n",
    "import os\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "\n",
    "# 1\n",
    "q_list = train['question'].tolist()\n",
    "a_list = train['answer'].tolist()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "q_ner_results = ner(q_list)\n",
    "a_ner_results = ner(a_list)\n",
    "\n",
    "\n",
    "num_q_ne_list = []\n",
    "num_a_ne_list = []\n",
    "\n",
    "for i in range(len(q_ner_results)):\n",
    "    num_q_ne_list.append(len(q_ner_results[i]))\n",
    "    num_a_ne_list.append(len(a_ner_results[i]))\n",
    "\n",
    "train['num_q_ne'] = num_q_ne_list\n",
    "train['num_a_ne'] = num_a_ne_list\n",
    "\n",
    "# 2\n",
    "q_ne_list = []\n",
    "a_ne_list = []\n",
    "\n",
    "for i in range(len(q_ner_results)):\n",
    "    tmp = []\n",
    "    for j in range(len(q_ner_results[i])):\n",
    "        tmp.append(q_ner_results[i][j]['word'])\n",
    "    q_ne_list.append(list(set(tmp)))\n",
    "\n",
    "for i in range(len(a_ner_results)):\n",
    "    tmp = []\n",
    "    for j in range(len(a_ner_results[i])):\n",
    "        tmp.append(a_ner_results[i][j]['word'])\n",
    "    a_ne_list.append(list(set(tmp)))\n",
    "        \n",
    "\n",
    "ne_unique_diff = []\n",
    "for i in range(len(q_ner_results)):\n",
    "    ne_unique_diff.append(len(set(a_ne_list[i]) - set(q_ne_list[i])))\n",
    "train['ne_unique_diff'] = ne_unique_diff\n",
    "\n",
    "# 3\n",
    "q_list = train['question'].tolist()\n",
    "a_list = train['answer'].tolist()\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "q_embs = model.encode(q_list)\n",
    "a_embs = model.encode(a_list)\n",
    "\n",
    "sim_list = []\n",
    "for i in range(len(q_embs)):\n",
    "    sim = 1 - cosine(q_embs[i], a_embs[i])\n",
    "    sim_list.append(sim)\n",
    "\n",
    "train['st_sim'] = sim_list\n",
    "\n",
    "# 4\n",
    "# Load BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')\n",
    "model = BertModel.from_pretrained('google-bert/bert-base-uncased')\n",
    "\n",
    "# Function to calculate BERT embeddings\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(1).detach().numpy()\n",
    "\n",
    "# Function to calculate semantic similarity\n",
    "def semantic_similarity(text1, text2):\n",
    "    embedding1 = get_bert_embedding(text1)\n",
    "    embedding2 = get_bert_embedding(text2)\n",
    "\n",
    "    return 1 - cosine(embedding1.flatten(), embedding2.flatten())\n",
    "\n",
    "q_list = train['question'].tolist()\n",
    "a_list = train['answer'].tolist()\n",
    "\n",
    "\n",
    "sim_list = []\n",
    "for i in range(len(q_list)):\n",
    "    sim = semantic_similarity(q_list[i], a_list[i])\n",
    "    sim_list.append(sim)\n",
    "\n",
    "train['bert_sim'] = sim_list\n",
    "\n",
    "# 5\n",
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')\n",
    "model = BertModel.from_pretrained('google-bert/bert-base-uncased')\n",
    "\n",
    "def get_embedding_variance(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.detach().numpy()\n",
    "    variances = np.var(embeddings, axis=1)\n",
    "    return np.mean(variances)\n",
    "\n",
    "info_ratio_list = []\n",
    "for i in range(len(q_list)):\n",
    "    question_variance = get_embedding_variance(q_list[i])\n",
    "    answer_variance = get_embedding_variance(a_list[i])\n",
    "    information_content_ratio = answer_variance / question_variance\n",
    "    info_ratio_list.append(information_content_ratio)\n",
    "\n",
    "train['bert_info_ratio'] = info_ratio_list\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "combined_texts = q_list + a_list\n",
    "tfidf_vectorizer.fit(combined_texts)\n",
    "\n",
    "info_ratio_list = []\n",
    "for i in range(len(q_list)):\n",
    "    tfidf_question = tfidf_vectorizer.transform([q_list[i]]).toarray()\n",
    "    tfidf_answer = tfidf_vectorizer.transform([a_list[i]]).toarray()\n",
    "    # Measure information content\n",
    "    information_content_ratio = np.sum(tfidf_answer) / np.sum(tfidf_question)\n",
    "    info_ratio_list.append(information_content_ratio)\n",
    "    \n",
    "train['tf-idf_info_ratio'] = info_ratio_list\n",
    "\n",
    "\n",
    "tokenizer_ = tiktoken.get_encoding(\"cl100k_base\")\n",
    "allowed_special = tokenizer_.special_tokens_set\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    chunk['len_q_character'] = chunk['question'].apply(len)\n",
    "    chunk['len_a_character'] = chunk['answer'].apply(len)\n",
    "\n",
    "    chunk['len_q_token'] = chunk['question'].apply(lambda x: len(tokenizer_.encode(x, allowed_special=allowed_special)))\n",
    "    chunk['len_a_token'] = chunk['answer'].apply(lambda x: len(tokenizer_.encode(x, allowed_special=allowed_special)))\n",
    "\n",
    "    return chunk\n",
    "\n",
    "\n",
    "chunk_size = math.ceil(len(train) / 80)  # Divide data into chunks for 80 CPU cores\n",
    "chunks = [train[i:i + chunk_size] for i in range(0, len(train), chunk_size)]\n",
    "\n",
    "with multiprocessing.Pool(processes=80) as pool:\n",
    "    processed_chunks = pool.map(process_chunk, chunks)\n",
    "\n",
    "train = pd.concat(processed_chunks)\n",
    "\n",
    "def calculate_entropy(sentence):\n",
    "    tokens = tokenizer_.encode(sentence, allowed_special=allowed_special)\n",
    "    #tokens = [tmp[i].form for i in range(len(tmp))]\n",
    "    token_freq = Counter(tokens)\n",
    "    total_tokens = len(tokens)\n",
    "    entropy = -sum((freq / total_tokens) * math.log2(freq / total_tokens) for freq in token_freq.values())\n",
    "    return entropy\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    chunk['q_entropy'] = chunk['question'].apply(calculate_entropy)\n",
    "    chunk['a_entropy'] = chunk['answer'].apply(calculate_entropy)\n",
    "    return chunk\n",
    "\n",
    "\n",
    "chunk_size = math.ceil(len(train) / 80)  # Divide data into chunks for 80 CPU cores\n",
    "chunks = [train[i:i + chunk_size] for i in range(0, len(train), chunk_size)]\n",
    "\n",
    "with multiprocessing.Pool(processes=80) as pool:\n",
    "    processed_chunks = pool.map(process_chunk, chunks)\n",
    "\n",
    "train = pd.concat(processed_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c1bfc-786c-43f3-9987-902e5bf3c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('./data_for_paepr/en_report_train_features.csv', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f270c-5bdf-44c2-95e1-e034c66da34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe043be-207f-4fe9-bbc4-4dad102b457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = valid\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tiktoken\n",
    "from collections import Counter\n",
    "import math\n",
    "#from kiwipiepy import Kiwi\n",
    "import os\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import math\n",
    "from kiwipiepy import Kiwi\n",
    "import os\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "\n",
    "# 1\n",
    "q_list = train['question'].tolist()\n",
    "a_list = train['answer'].tolist()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "q_ner_results = ner(q_list)\n",
    "a_ner_results = ner(a_list)\n",
    "\n",
    "\n",
    "num_q_ne_list = []\n",
    "num_a_ne_list = []\n",
    "\n",
    "for i in range(len(q_ner_results)):\n",
    "    num_q_ne_list.append(len(q_ner_results[i]))\n",
    "    num_a_ne_list.append(len(a_ner_results[i]))\n",
    "\n",
    "train['num_q_ne'] = num_q_ne_list\n",
    "train['num_a_ne'] = num_a_ne_list\n",
    "\n",
    "# 2\n",
    "q_ne_list = []\n",
    "a_ne_list = []\n",
    "\n",
    "for i in range(len(q_ner_results)):\n",
    "    tmp = []\n",
    "    for j in range(len(q_ner_results[i])):\n",
    "        tmp.append(q_ner_results[i][j]['word'])\n",
    "    q_ne_list.append(list(set(tmp)))\n",
    "\n",
    "for i in range(len(a_ner_results)):\n",
    "    tmp = []\n",
    "    for j in range(len(a_ner_results[i])):\n",
    "        tmp.append(a_ner_results[i][j]['word'])\n",
    "    a_ne_list.append(list(set(tmp)))\n",
    "        \n",
    "\n",
    "ne_unique_diff = []\n",
    "for i in range(len(q_ner_results)):\n",
    "    ne_unique_diff.append(len(set(a_ne_list[i]) - set(q_ne_list[i])))\n",
    "train['ne_unique_diff'] = ne_unique_diff\n",
    "\n",
    "# 3\n",
    "q_list = train['question'].tolist()\n",
    "a_list = train['answer'].tolist()\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "q_embs = model.encode(q_list)\n",
    "a_embs = model.encode(a_list)\n",
    "\n",
    "sim_list = []\n",
    "for i in range(len(q_embs)):\n",
    "    sim = 1 - cosine(q_embs[i], a_embs[i])\n",
    "    sim_list.append(sim)\n",
    "\n",
    "train['st_sim'] = sim_list\n",
    "\n",
    "# 4\n",
    "# Load BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')\n",
    "model = BertModel.from_pretrained('google-bert/bert-base-uncased')\n",
    "\n",
    "# Function to calculate BERT embeddings\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(1).detach().numpy()\n",
    "\n",
    "# Function to calculate semantic similarity\n",
    "def semantic_similarity(text1, text2):\n",
    "    embedding1 = get_bert_embedding(text1)\n",
    "    embedding2 = get_bert_embedding(text2)\n",
    "\n",
    "    return 1 - cosine(embedding1.flatten(), embedding2.flatten())\n",
    "\n",
    "q_list = train['question'].tolist()\n",
    "a_list = train['answer'].tolist()\n",
    "\n",
    "\n",
    "sim_list = []\n",
    "for i in range(len(q_list)):\n",
    "    sim = semantic_similarity(q_list[i], a_list[i])\n",
    "    sim_list.append(sim)\n",
    "\n",
    "train['bert_sim'] = sim_list\n",
    "\n",
    "# 5\n",
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')\n",
    "model = BertModel.from_pretrained('google-bert/bert-base-uncased')\n",
    "\n",
    "def get_embedding_variance(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.detach().numpy()\n",
    "    variances = np.var(embeddings, axis=1)\n",
    "    return np.mean(variances)\n",
    "\n",
    "info_ratio_list = []\n",
    "for i in range(len(q_list)):\n",
    "    question_variance = get_embedding_variance(q_list[i])\n",
    "    answer_variance = get_embedding_variance(a_list[i])\n",
    "    information_content_ratio = answer_variance / question_variance\n",
    "    info_ratio_list.append(information_content_ratio)\n",
    "\n",
    "train['bert_info_ratio'] = info_ratio_list\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "combined_texts = q_list + a_list\n",
    "tfidf_vectorizer.fit(combined_texts)\n",
    "\n",
    "info_ratio_list = []\n",
    "for i in range(len(q_list)):\n",
    "    tfidf_question = tfidf_vectorizer.transform([q_list[i]]).toarray()\n",
    "    tfidf_answer = tfidf_vectorizer.transform([a_list[i]]).toarray()\n",
    "    # Measure information content\n",
    "    information_content_ratio = np.sum(tfidf_answer) / np.sum(tfidf_question)\n",
    "    info_ratio_list.append(information_content_ratio)\n",
    "    \n",
    "train['tf-idf_info_ratio'] = info_ratio_list\n",
    "\n",
    "\n",
    "# 7\n",
    "tokenizer_ = tiktoken.get_encoding(\"cl100k_base\")\n",
    "allowed_special = tokenizer_.special_tokens_set\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    chunk['len_q_character'] = chunk['question'].apply(len)\n",
    "    chunk['len_a_character'] = chunk['answer'].apply(len)\n",
    "\n",
    "    chunk['len_q_token'] = chunk['question'].apply(lambda x: len(tokenizer_.encode(x, allowed_special=allowed_special)))\n",
    "    chunk['len_a_token'] = chunk['answer'].apply(lambda x: len(tokenizer_.encode(x, allowed_special=allowed_special)))\n",
    "\n",
    "    return chunk\n",
    "\n",
    "\n",
    "chunk_size = math.ceil(len(train) / 80)  # Divide data into chunks for 80 CPU cores\n",
    "chunks = [train[i:i + chunk_size] for i in range(0, len(train), chunk_size)]\n",
    "\n",
    "with multiprocessing.Pool(processes=80) as pool:\n",
    "    processed_chunks = pool.map(process_chunk, chunks)\n",
    "\n",
    "train = pd.concat(processed_chunks)\n",
    "\n",
    "# 8\n",
    "def calculate_entropy(sentence):\n",
    "    tokens = tokenizer_.encode(sentence, allowed_special=allowed_special)\n",
    "    #tokens = [tmp[i].form for i in range(len(tmp))]\n",
    "    token_freq = Counter(tokens)\n",
    "    total_tokens = len(tokens)\n",
    "    entropy = -sum((freq / total_tokens) * math.log2(freq / total_tokens) for freq in token_freq.values())\n",
    "    return entropy\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    chunk['q_entropy'] = chunk['question'].apply(calculate_entropy)\n",
    "    chunk['a_entropy'] = chunk['answer'].apply(calculate_entropy)\n",
    "    return chunk\n",
    "\n",
    "\n",
    "chunk_size = math.ceil(len(train) / 80)  # Divide data into chunks for 80 CPU cores\n",
    "chunks = [train[i:i + chunk_size] for i in range(0, len(train), chunk_size)]\n",
    "\n",
    "with multiprocessing.Pool(processes=80) as pool:\n",
    "    processed_chunks = pool.map(process_chunk, chunks)\n",
    "\n",
    "train = pd.concat(processed_chunks)\n",
    "\n",
    "\n",
    "train.to_csv('./data_for_paepr/valid.csv', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6edbb0c-6c40-49b5-af59-8cd278fdaaf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2f04f8-0b7d-4ac1-8481-a0bc601797c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d672dfb1-a150-47ed-b30c-dddfad97027d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
